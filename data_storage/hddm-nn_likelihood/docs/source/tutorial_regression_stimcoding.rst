.. index:: Tutorial
.. _chap_tutorial_hddm_regression:

Stimulus coding with HDDMRegression
###################################

Note: This tutorial is more advanced. If you are just starting you might want
to head to the :ref:`demo <chap_demo>` instead.

In some situations it is useful to fix the magnitude of parameters
across stimulus types while also forcing them to have different
directions. For example, an independent variable could influence both
the drift rate ``v`` and the response bias ``z``. A specific example is an
experiment on face-house discrimination with different difficulty
levels, where the drift-rate is smaller when the task is more
difficult and where the bias to responding house is larger when the
task is more difficult.  One way to analyze the effect of difficulty
on drift rate and bias in such an experiment is to estimate one drift
rate ``v`` for each level, and a response bias ``z`` such that the bias for
houses-stimuli is ``z`` and the bias for face stimuli is ``1-z`` (``z = .5``
for unbiased decisions in ``HDDM``).

The following example describes how to generate simulated data for
such an experiment, how to set up the analysis with ``HDDMRegression``,
and compares true parameter values with those estimated with
``HDDMRegression``.

Model Recovery Test for HDDMRegression
**************************************

The test is performed with simulated data for an experiment with one
independent variable with three levels (e.g. three levels of
difficulty) which influence both drift rate ``v`` and bias ``z``. Responses
are "accuracy coded", i.e. correct responses are coded ``1`` and incorrect
responses ``0``. Further, stimulus coding of the parameter ``z`` is
implemented. "stimulus coding" of ``z`` means that we want to fit a model
in which the magnitude of the bias is the same for the two stimuli,
but its direction "depends on" the presented stimulus (e.g. faces or
house in a face-house discrimination task). Note that this does not
mean that we assume that decision makers adjust their bias after
having seen the stimulus. Rather, we want to measure response-bias (in
favor of face or house) while assuming the same drift rate for both
stimuli. We can achieve this for accuracy coded data by modeling the
bias as moved towards the correct response boundary for one stimulus
(e.g. ``z = .6`` for houses) and away from the correct response boundary
for the other stimulus (``1-z = .4`` for faces).

First, we need to import the required python modules.
::

    import hddm
    from patsy import dmatrix  # for generation of (regression) design matrices
    import numpy as np         # for basic matrix operations
    from pandas import Series  # to manipulate data-frames generated by hddm

We save the output of stdout to the file ``ModelRecoveryOutput.txt``.
::

    import sys
    sys.stdout = open('ModelRecoveryOutput.txt', 'w')

Creating simulated data for the experiment
******************************************

Next we set the number of subjects and the number of trials per level
for the simulated experiment ::

    n_subjects = 10
    trials_per_level = 150 # and per stimulus

Next we set up parameters of the drift diffusion process for the three
levels and the first stimulus. As desribed earlier ``v`` and ``z`` change
accross levels ::

    level1a = {'v':.3, 'a':2, 't':.3, 'sv':0, 'z':.5, 'sz':0, 'st':0}
    level2a = {'v':.4, 'a':2, 't':.3, 'sv':0, 'z':.6, 'sz':0, 'st':0}
    level3a = {'v':.5, 'a':2, 't':.3, 'sv':0, 'z':.7, 'sz':0, 'st':0}

Now we generate the data for stimulus A

::

    data_a, params_a = hddm.generate.gen_rand_data({'level1': level1a,
                                                    'level2': level2a,
						    'level3': level3a},
						    size=trials_per_level,
						    subjs=n_subjects)

Next come the parameters for the second stimulus, where ``v`` is the same
as for the first stimulus. This is different for ``z``. In particular:
``z(stimulus_b) = 1 - z(stimulus_a)``. As a result, responses are
altogether biased towards responding A. Because we use accuracy coded
data, stimulus A is biased towards correct responses, and stimulus B
towards incorrect responses.  ::

    level1b = {'v':.3, 'a':2, 't':.3,'sv': 0, 'z':.5, 'sz': 0, 'st': 0}
    level2b = {'v':.4, 'a':2, 't':.3,'sv': 0, 'z':.4, 'sz': 0, 'st': 0}
    level3b = {'v':.5, 'a':2, 't':.3,'sv': 0, 'z':.3, 'sz': 0, 'st': 0}

Now we generate the data for stimulus B

::

    data_b, params_b = hddm.generate.gen_rand_data({'level1': level1b,
                                                    'level2': level2b,
                                                    'level3': level3b},
						    size=trials_per_level,
						    subjs=n_subjects)

We add a column to the ``DataFrame`` identifying stimulus A as 1 and stimulus B as 2.

::

    data_a['stimulus'] = Series(np.ones((len(data_a))), index=data_a.index)
    data_b['stimulus'] = Series(np.ones((len(data_b)))*2, index=data_a.index)

Now we merge the data for stimulus A and B

::

    mydata = data_a.append(data_b, ignore_index=True)

Setting up the HDDM regression model
************************************

The parameter ``z`` is bound between ``0`` and ``1``, but the standard
linear regression does not generate values between ``0`` and
``1``. Therefore we use a link-function, here the inverse logit
``1/(1+exp(-x))``, which transforms values between plus and minus
infinity into values ranging from (just above) ``0`` to (nearly)
``1``. [If this reminds you of link functions for logistic regressions,
thatâ€™s correct].

Next we need to insure that the bias is ``z`` for one stimulus and ``1-z``
for the other stimulus. To achieve this, we can simply multiply the
regression output for one stimulus with ``-1``. This is implemented here
by dot-multiplying the regression output "x" (which is an array) with
equally sized array "stim", which is 1 for all stimulus A trials
and -1 for stimulus B trials. We use the ``patsy`` command ``dmatrix`` to
generate such an array from the stimulus column of our simulated data
::

    def z_link_func(x, data=mydata):
        stim = (np.asarray(dmatrix('0 + C(s, [[1], [-1]])',
	                           {'s': data.stimulus.ix[x.index]}))
	)
        return 1 / (1 + np.exp(-(x * stim)))

Now we set up the regression models for ``z`` and ``v`` and also include the
link functions The relevant string here used by ``patsy`` is '1 +
C(condition)'. This will generate a design matrix with an intercept
(that's what the '1' is for) and two dummy variables for remaining
levels. (The column in which the levels are coded has the default name
'condition'):
::

    z_reg = {'model': 'z ~ 1 + C(condition)', 'link_func': z_link_func}

For ``v`` the link function is simply ``x = x``, because no transformations is
needed. [However, you could also analyze this experiment with response
coded data. Then you would not stimulus code ``z`` but ``v`` and you would
have to multiply the ``v`` for one condition with ``-1``, with a link function
like the one for ``z`` above, but with out the additional logit transform
]:
::

    v_reg = {'model': 'v ~ 1 + C(condition)', 'link_func': lambda x: x}

Now we can finally put the regression description for the hddm model
together. The general for this is ``[{'model': 'outcome_parameter ~ patsy_design_string', 'link_func': your_link_function }, {...}, ...]``
::

    reg_descr = [z_reg, v_reg]

The last step before running the model is to construct the complete hddm regression model by adding data etc.
::

    m_reg = hddm.HDDMRegressor(mydata, reg_descr, include='z')

Now we start the model, and wait for a while (you can go and get
several coffees, or read a paper). Sampling 20000 samples for the
example experiment described here took 77 minutes on a macbook pro
with a 2.66 GHz Intel Core i7 (for a real experiment with data that
are certainly noisier than the simulated data one should sample ca 10
times as many samples).
::

    m_reg.sample(5000, burn=200)

Comparing generative and recovered model parameters
***************************************************

First we print the model stats
::

    m_reg.print_stats()

Here is the relevant output for our purposes:
.. parsed-literal::

    parameter			mean       std      2.5q       25q       50q       75q      97.5q

    z_Intercept			-0.04459  0.148731 -0.348728 -0.141392 -0.045055  0.046041  0.271

    z_C(condition)[T.level2]	0.395524  0.049708  0.304394  0.354014  0.402072  0.426116  0.496

    z_C(condition)[T.level3]	0.818458  0.049148  0.712337  0.788209  0.820972  0.850570  0.903

    v_Intercept			0.269770  0.058421  0.151004  0.237380  0.271991  0.303675  0.380

    v_C(condition)[T.level2]	0.159221  0.051821  0.065206  0.123976  0.157030  0.192976  0.271

    v_C(condition)[T.level3]	0.250912  0.059487  0.152756  0.203228  0.251347  0.290904  0.373

Lets first look at ``v``. For ``level1`` this is just the
intercept. The value of ``.27`` is in the ball park of the true value
of ``.3``. The fit is not perfect, but running a longer chain might
help (we are ignoring sophisticated checks of model convergence for
this example here). To get the values of ``v`` for levels 2 and 3, we
have to add the respective parameters (``0.16`` and ``.25``) to the
intercept value. The resulting values of ``.43`` and ``.52`` are again
close enough to the true values of ``.4`` and ``.5``. To get the
estimated ``z`` value we first need to "convert" the regression value
with our link function. For level 1 this is ``1/(1+exp(-(-0.044))) =
.48``, which is close to the true value of ``.5``. For level 2 this is
``1/(1+exp(-(-0.044+0.396))) = .59``, again close to the true value of
``.6``, as is the case for level 3 (``.68`` vs. ``.7``).  In sum,
``HDDMRegression`` easily recovered the right order of the parameters
``z``. The recovered parameter values are also close to the true
parameter values. The deviations show that (a) we should maybe run
longer MCMC chains and, more importantly, (b) that for the relatively
small differences in DDM parameters we tested here a larger experiment
(i.e. more trials per conditions or more participants) would be
better.
